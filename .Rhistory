poops <- ExtractKeyword(tweets, "poop")
GetNumKeywordsByDays(poops, 4)
pees <- ExtractKeyword(tweets, "pee")
GetNumKeywordsByDays(pees, 4)
# uncomment the lines below to install packages
#install.packages("twitteR")
#install.packages("stringr")
#install.packages("dplyr")
library("twitteR")
library("stringr")
library("dplyr")
kIgnoreTweet <- "update:|nobot:"
GetTweets <- function(handle, n = 1000) {
# Retrieve tweets.
#
# Args:
#   handle: Twitter handle.
#   n: Number of tweets to retrieve (default: 1000).
#
# Returns:
#   Tweet data frame with text.orig and created.orig columns.
timeline <- userTimeline(handle, n = n)
tweets <- sapply(timeline, function(x) {
c(x$getText(), x$getCreated())
})
tweets <- data.frame(t(tweets))
names(tweets) <- c("text.orig", "created.orig")
tweets$text <- tolower(tweets$text.orig)
tweets$created <- as.POSIXct(as.numeric(as.vector(tweets$created.orig)), origin="1970-01-01")
arrange(tweets, created)
}
ExtractVolumes <- function(tweets) {
# Extract volumes from tweet data frame.
#
# Args:
#   tweets: Tweet data frame.
#
# Returns:
#   Data frame containing volume information.
volumes <- tweets[grepl("\\d+ ?ml", tweets$text) & !grepl(kIgnoreTweet, tweets$text), ]
volumes$volume <- as.numeric(str_match(volumes$text, "(\\d+) ?ml")[, 2])
volumes$milktype <- rep("formula", nrow(volumes))
volumes$milktype[grep("bm|breast", volumes$text)] <- "breast"
volumes
}
GetVolumeSumByDate <- function(volumes, dt) {
# Extract volume info for a specific date or date range.
#
# Args:
#   volumes: Volume data frame.
#   dt: Date or date range to extract details for.
#
# Returns:
#   Data frame containing volume information for a specific date or date range.
result <- sapply(dt, function(x) {
lower <- volumes$created - x >= 0
upper <- volumes$created - (x + 86400) < 0
sum(volumes[lower & upper, "volume"])
})
names(result) <- dt
result
}
GetVolumeSumByDays <- function(volumes, n = 1) {
# Extract volume info for a specific number of days in the past.
#
# Args:
#   volumes: Volume data frame.
#   n: Number of days (default: 1).
#
# Returns:
#   Data frame containing volume information for a specific number of days in the past.
today <- as.POSIXct(as.character(Sys.Date()))
GetVolumeSumByDate(volumes,
seq.POSIXt(from = today - ((n - 1) * 86400),
to = today,
by = "days"))
}
GetVolumeMilkTypeSumByDate <- function(volumes, dt, milktype) {
# Extract milk type info for a specific date or date range.
#
# Args:
#   volumes: Volume data frame.
#   dt: Date or date range to extract details for.
#
# Returns:
#   Data frame containing milk type information for a specific date or date range.
result <- sapply(dt, function(x) {
lower <- volumes$created - x >= 0
upper <- volumes$created - (x + 86400) < 0
sum(volumes[lower & upper & volumes$milktype == milktype, "volume"])
})
names(result) <- dt
result
}
GetVolumeMilkTypeSumByDays <- function(volumes, n = 1, milktype) {
# Extract milk type info for a specific number of days in the past.
#
# Args:
#   volumes: Volume data frame.
#   n: Number of days (default: 1).
#
# Returns:
#   Data frame containing milk type information for a specific number of days in the past.
today <- as.POSIXct(as.character(Sys.Date()))
GetVolumeMilkTypeSumByDate(volumes,
seq.POSIXt(from = today - ((n - 1) * 86400),
to = today,
by = "days"),
milktype)
}
ExtractKeyword <- function(tweets, keyword) {
# Extract data with keyword from tweet data frame.
#
# Args:
#   tweets: Tweet data frame.
#   keyword: Keyword to search for.
#
# Returns:
#   Data frame of tweets containing keyword.
tweets[grepl(keyword, tweets$text) & !grepl(kIgnoreTweet, tweets$text), ]
}
GetNumKeywordsByDate <- function(dataset, dt) {
# Extract number of keywords for a specific date or date range.
#
# Args:
#   dataset: Keyword data frame.
#   dt: Date or date range to extract details for.
#
# Returns:
#   Data frame containing keyword for a specific date or date range.
result <- sapply(dt, function(x) {
lower <- dataset$created - x >= 0
upper <- dataset$created - (x + 86400) < 0
nrow(dataset[lower & upper, ])
})
names(result) <- dt
result
}
GetNumKeywordsByDays <- function(dataset, n = 1) {
# Extract keyword info for a specific number of days in the past.
#
# Args:
#   volumes: Keyword data frame.
#   n: Number of days (default: 1).
#
# Returns:
#   Data frame containing keyword information for a specific number of days in the past.
today <- as.POSIXct(as.character(Sys.Date()))
GetNumKeywordsByDate(dataset,
seq.POSIXt(from = today - ((n - 1) * 86400),
to = today,
by = "days"))
}
consumerKey <- "5CMLaLXybPuWxFAZjkF4F2kAa"
consumerSecret <- "CRlXBGEsyGMIRoQXgV57W8CloYZnyTTHt0bWuammW31t7vE7N2"
accessToken <- "287380276-5YbMFS2TzKaierpQsbrHehy25bbjMpsGvaNyxgJy"
accessTokenSecret <- "NlYIlEKMKTeT7Qa1f33T31TIEwqo6xXDFmE6PUiJPIPpr"
handle <- "billzichos"
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
tweets <- GetTweets(handle, 100)
volumes <- ExtractVolumes(tweets)
GetVolumeSumByDays(volumes, 4)
round(GetVolumeMilkTypeSumByDays(volumes, 4, "breast") / GetVolumeSumByDays(volumes, 4) * 100)
pumps <- ExtractKeyword(tweets, "pump")
GetNumKeywordsByDays(pumps, 4)
poops <- ExtractKeyword(tweets, "poop")
GetNumKeywordsByDays(poops, 4)
pees <- ExtractKeyword(tweets, "pee")
GetNumKeywordsByDays(pees, 4)
tweets
bge.list <- searchTwitter('@MyBGE', n=1000, cainfo="cacert.pem")
bge.list <- searchTwitter('@MyBGE', n=1000)
bge.list
getwd()
file("test.ics")
f <- file("test.ics")
writeLines("line1",f)
close f
close(f)
f <- file("test.ics")
writeLines("line 2 hello world", f)
close(f)
writeLines(c('line 1','line 2 hello world'), f)
f <- file("test.ics")
writeLines(c('line 1','line 2 hello world'), f)
close(f)
subject <- 'test subject'
line1 <- 'BEGIN:VCALENDAR'
line2 <- 'VERSION:2.0'
line3 <- 'BEGIN:VEVENT'
line4 <- paste('SUMMARY:', subject, sep='')
line5 <- 'END:VEVENT'
line6 <- 'END:VCALENDAR'
f <- file("test.ics")
writeLines(c(line1, line2, line3, line4, line5, line6), f)
close(f)
line01 <- 'BEGIN:VCALENDAR'
line02 <- 'VERSION:2.0'
line03 <- 'PRODID:'
line04 <- 'BEGIN:VEVENT'
line05 <- 'UID:'
line06 <- 'DTSTAMP:20150530T170000Z'
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com'
line08 <- 'DTSTART:20150530T170000Z'
line09 <- 'DTEND:20150530T180000Z'
line10 <- 'SUMMARY:[enter subject here]'
line11 <- 'DESCRIPTION:[enter description here]'
line12 <- 'END:VEVENT'
line13 <- 'END:VCALENDAR'
f <- file("test.ics")
writeLines(
c(line01, line02, line03, line04, line05, line06, line07, line08, line09, line10, line11, line12, line13),
f)
close(f)
line01 <- 'BEGIN:VCALENDAR'
line02 <- 'VERSION:2.0'
line03 <- 'PRODID:'
line04 <- 'BEGIN:VEVENT'
line05 <- 'UID:bill.zichos'
line06 <- 'DTSTAMP:20150530T170000Z'
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com'
line08 <- 'DTSTART:20150530T170000Z'
line09 <- 'DTEND:20150530T180000Z'
line10 <- 'SUMMARY:[enter subject here]'
line11 <- 'DESCRIPTION:[enter description here]'
line12 <- 'END:VEVENT'
line13 <- 'END:VCALENDAR'
f <- file("test.ics")
writeLines(
c(line01, line02, line03, line04, line05, line06, line07, line08, line09, line10, line11, line12, line13),
f)
close(f)
line03 <- 'PRODID:R'
line01 <- 'BEGIN:VCALENDAR'
line02 <- 'VERSION:2.0'
line03 <- 'PRODID:R'
line04 <- 'BEGIN:VEVENT'
line05 <- 'UID:bill.zichos'
line06 <- 'DTSTAMP:20150530T170000Z'
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com'
line08 <- 'DTSTART:20150530T170000Z'
line09 <- 'DTEND:20150530T180000Z'
line10 <- 'SUMMARY:[enter subject here]'
line11 <- 'DESCRIPTION:[enter description here]'
line12 <- 'END:VEVENT'
line13 <- 'END:VCALENDAR'
f <- file("test.ics")
writeLines(
c(line01, line02, line03, line04, line05, line06, line07, line08, line09, line10, line11, line12, line13),
f)
close(f)
f <- file("test.ics")
writeLines(c(line01 <- 'BEGIN:VCALENDAR'
line02 <- 'VERSION:2.0'
line03 <- 'PRODID:R'
line04 <- 'BEGIN:VEVENT'
line05 <- 'UID:bill.zichos'
line06 <- 'DTSTAMP:20150530T170000Z'
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com'
line08 <- 'DTSTART:20150530T170000Z'
line09 <- 'DTEND:20150530T180000Z'
line10 <- paste('SUMMARY:', subject, sep='')
line11 <- paste('DESCRIPTION:', description, sep='')
line12 <- 'END:VEVENT'
line13 <- 'END:VCALENDAR'
), f)
close(f)
description <- '[enter description here]'
f <- file("test.ics")
writeLines(c(line01 <- 'BEGIN:VCALENDAR'
line02 <- 'VERSION:2.0'
line03 <- 'PRODID:R'
line04 <- 'BEGIN:VEVENT'
line05 <- 'UID:bill.zichos'
line06 <- 'DTSTAMP:20150530T170000Z'
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com'
line08 <- 'DTSTART:20150530T170000Z'
line09 <- 'DTEND:20150530T180000Z'
line10 <- paste('SUMMARY:', subject, sep='')
line11 <- paste('DESCRIPTION:', description, sep='')
line12 <- 'END:VEVENT'
line13 <- 'END:VCALENDAR'
), f)
close(f)
f <- file("test.ics")
writeLines(c(line01 <- 'BEGIN:VCALENDAR',
line02 <- 'VERSION:2.0',
line03 <- 'PRODID:R',
line04 <- 'BEGIN:VEVENT',
line05 <- 'UID:bill.zichos',
line06 <- 'DTSTAMP:20150530T170000Z',
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com',
line08 <- 'DTSTART:20150530T170000Z',
line09 <- 'DTEND:20150530T180000Z',
line10 <- paste('SUMMARY:', subject, sep=''),
line11 <- paste('DESCRIPTION:', description, sep=''),
line12 <- 'END:VEVENT',
line13 <- 'END:VCALENDAR'
), f)
close(f)
f <- file("test.ics")
writeLines(c(line01 <- 'BEGIN:VCALENDAR',
line02 <- 'VERSION:2.0',
line03 <- 'PRODID:R',
line04 <- 'BEGIN:VEVENT',
line05 <- 'UID:bill.zichos',
line06 <- 'DTSTAMP:20150530T000000',
line07 <- 'ORGANIZER;CN=Bill Zichos:MAILTO:billzichos@hotmail.com',
line08 <- 'DTSTART:20150530T000000',
line09 <- 'DTEND:20150531T000000',
line10 <- paste('SUMMARY:', subject, sep=''),
line11 <- paste('DESCRIPTION:', description, sep=''),
line12 <- 'END:VEVENT',
line13 <- 'END:VCALENDAR'
), f)
close(f)
today()
system.time()
date()
month(date())
#Week 2 Quiz
library("httr")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
myapp <- oauth_app("github",
key = "b278d64147503fcd381b",
secret = "b98c9972b60884be3cf4c187bd24dc6cc783b315")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app("github",
key = "56b637a5baffac62cad9",
secret = "8e107541ae1791259e9987d544ca568633da2ebf")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
setwd("~/Private-Github-Files")
library(dplyr)
# setwd("~/Private-Github-Files")
snapshotFilename <- "OpenDeals20150601.csv"
results <- read.csv("~//Private-Github-Files//results.csv")
# str(results)
snapshot <- read.csv(paste("~//Private-Github-Files//", snapshotFilename, sep =""))
# str(snapshot)
# summary(snapshot)
comb <- merge(snapshot, results, by.x = 'Deal.Number', by.y = 'Deal.Number')
# summary(comb)
# str(comb)
comb$ClosedAsWon[comb$Status=="Lost"] <- 0
comb$ClosedAsWon[comb$Status=="Won"] <- 1
completeDataset <- select(comb, 1:21)
# str(completeDataset)
# str(completeDataset$ClosedAsWon)
# summary(completeDataset)
# summary(completeDataset$ClosedAsWon)
write.csv(completeDataset,paste("~//Private-Github-Files//",snapshotFilename, sep = ""), row.names = FALSE)
str(completeDataset$ClosedAsWon)
str(completeDataset)
summary(completeDataset$ClosedAsWon)
#Update the periodic snapshot with Results if known
library(dplyr)
# setwd("~/Private-Github-Files")
snapshotFilename <- "OpenDeals20150508.csv"
results <- read.csv("~//Private-Github-Files//results.csv")
# str(results)
snapshot <- read.csv(paste("~//Private-Github-Files//", snapshotFilename, sep =""))
# str(snapshot)
# summary(snapshot)
comb <- merge(snapshot, results, by.x = 'Deal.Number', by.y = 'Deal.Number')
# summary(comb)
# str(comb)
comb$ClosedAsWon[comb$Status=="Lost"] <- 0
comb$ClosedAsWon[comb$Status=="Won"] <- 1
completeDataset <- select(comb, 1:21)
# str(completeDataset)
# str(completeDataset$ClosedAsWon)
# summary(completeDataset)
# summary(completeDataset$ClosedAsWon)
write.csv(completeDataset,paste("~//Private-Github-Files//",snapshotFilename, sep = ""), row.names = FALSE)
wd <- "~/GitHub/San-Francisco-Crime-Classification"
setwd <- wd
wd()
downloadKaggle("sf-crime","test.csv.zip")
downloadKaggle("sf-crime","train.csv.zip")
source("~/GitHub/Get-Raw-Data/download.R")
wd <- "~/GitHub/San-Francisco-Crime-Classification"
setwd <- wd
downloadKaggle("sf-crime","train.csv.zip")
downloadKaggle("sf-crime","test.csv.zip")
downloadKaggle("sf-crime","sampleSubmission.csv.zip")
setwd("~/GitHub/San-Francisco-Crime-Classification")
setwd(wd)
downloadKaggle("sf-crime","train.csv.zip")
downloadKaggle("sf-crime","test.csv.zip")
downloadKaggle("sf-crime","sampleSubmission.csv.zip")
wd <- "~/GitHub/First-Steps-With-Julia"
setwd(wd)
wd <- "~/GitHub/First-Steps-With-Julia"
setwd(wd)
# The following files are provided
#   - train.zip
#   - test.zip
#   - trainResized.zip
#   - testResized.zip
#   - trainLabels.csv
#   - resizeData.py
#   - sampleSubmission.csv
#   - source-code-files.zip
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("street-view-getting-started-with-julia","train.zip")
downloadKaggle("street-view-getting-started-with-julia","test.zip")
downloadKaggle("street-view-getting-started-with-julia","trainResized.zip")
downloadKaggle("street-view-getting-started-with-julia","testResized.zip")
downloadKaggle("street-view-getting-started-with-julia","trainLabels.csv")
downloadKaggle("street-view-getting-started-with-julia","resizeData.zip")
downloadKaggle("street-view-getting-started-with-julia","sampleSubmission.csv")
downloadKaggle("street-view-getting-started-with-julia","source-code-files.zip")
downloadKaggle("street-view-getting-started-with-julia","resizeData.py")
wd <- "~/GitHub/Facial-Keypoints-Detection"
setwd(wd)
# The following files are provided
#   - training.zip
#   - test.zip
#   - SampleSubmission.csv
#   - IdLookupTable.csv
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("facial-keypoints-detection","training.zip")
downloadKaggle("facial-keypoints-detection","test.zip")
downloadKaggle("facial-keypoints-detection","SampleSubmission.csv")
downloadKaggle("facial-keypoints-detection","IdLookupTable.csv")
wd <- "~GitHub\\Titanic-Machine-Learning-From-Disaster"
setwd(wd)
# The following files are provided
#   - train.csv
#   - gendermodel.csv
#   - genderclassmodel.csv
#   - test.csv
#   - gendermodel.py
#   - genderclassmodel.py
#   - myfirstforest.py
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("titanic","train.csv")
downloadKaggle("titanic","gendermodel.csv")
downloadKaggle("titanic","genderclassmodel.csv")
downloadKaggle("titanic","test.csv")
downloadKaggle("titanic","gendermodel.py")
downloadKaggle("titanic","genderclassmodel.py")
downloadKaggle("titanic","myfirstforest.py")
# The following files are provided
#   - train.csv
#   - gendermodel.csv
#   - genderclassmodel.csv
#   - test.csv
#   - gendermodel.py
#   - genderclassmodel.py
#   - myfirstforest.py
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("titanic","train.csv")
downloadKaggle("titanic","gendermodel.csv")
downloadKaggle("titanic","genderclassmodel.csv")
downloadKaggle("titanic","test.csv")
downloadKaggle("titanic","gendermodel.py")
downloadKaggle("titanic","genderclassmodel.py")
downloadKaggle("titanic","myfirstforest.py")
wd <- "~GitHub\\Titanic-Machine-Learning-From-Disaster"
setwd(wd)
setwd("~/GitHub/Titanic-Machine-Learning-From-Disaster")
wd <- "~\\GitHub\\Titanic-Machine-Learning-From-Disaster"
setwd(wd)
# The following files are provided
#   - train.csv
#   - gendermodel.csv
#   - genderclassmodel.csv
#   - test.csv
#   - gendermodel.py
#   - genderclassmodel.py
#   - myfirstforest.py
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("titanic","train.csv")
downloadKaggle("titanic","gendermodel.csv")
downloadKaggle("titanic","genderclassmodel.csv")
downloadKaggle("titanic","test.csv")
downloadKaggle("titanic","gendermodel.py")
downloadKaggle("titanic","genderclassmodel.py")
downloadKaggle("titanic","myfirstforest.py")
wd <- "~/GitHub/Facial-Keypoints-Detection"
setwd(wd)
# The following files are provided
#   - training.zip
#   - test.zip
#   - SampleSubmission.csv
#   - IdLookupTable.csv
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("facial-keypoints-detection","training.zip")
downloadKaggle("facial-keypoints-detection","test.zip")
downloadKaggle("facial-keypoints-detection","SampleSubmission.csv")
downloadKaggle("facial-keypoints-detection","IdLookupTable.csv")
wd <- "~/GitHub/Digit-Recognizer"
setwd(wd)
# The following files are provided
#   - train.csv
#   - test.csv
source("~/GitHub/Get-Raw-Data/download.R")
downloadKaggle("digit-recognizer","train.csv")
downloadKaggle("digit-recognizer","test.csv")
